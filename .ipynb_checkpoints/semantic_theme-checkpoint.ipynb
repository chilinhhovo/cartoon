{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "822df9da-a9ee-404e-946f-d14dbee4fc95",
   "metadata": {},
   "source": [
    "## sentence transformers + Cluster the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aad7d5a-7977-4e2d-af7b-254701cf6601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf1c2e7b92744298b63df49cb84d1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: semantic_similarity_with_themes.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"semantic_similarity_with_matched_category.csv\")\n",
    "\n",
    "# Combine captions into one column\n",
    "all_captions = pd.concat([\n",
    "    df[\"human_caption\"],\n",
    "    df[\"ai_caption\"]\n",
    "]).dropna().unique().tolist()\n",
    "\n",
    "# Load sentence embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "caption_embeddings = model.encode(all_captions, show_progress_bar=True)\n",
    "\n",
    "# Use PCA to reduce dimension for faster clustering (optional)\n",
    "pca = PCA(n_components=50)\n",
    "reduced_embeddings = pca.fit_transform(caption_embeddings)\n",
    "\n",
    "# Cluster using KMeans\n",
    "NUM_CLUSTERS = 10  # you can adjust this!\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
    "labels = kmeans.fit_predict(reduced_embeddings)\n",
    "\n",
    "# Create DataFrame with results\n",
    "theme_df = pd.DataFrame({\n",
    "    \"caption\": all_captions,\n",
    "    \"theme_cluster\": labels\n",
    "})\n",
    "\n",
    "# Extract keywords for each theme\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(theme_df[\"caption\"])\n",
    "\n",
    "# Map cluster numbers to top keywords (for human-readable themes)\n",
    "keywords_per_theme = []\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cluster_texts = tfidf_matrix[theme_df[\"theme_cluster\"] == i]\n",
    "    mean_tfidf = cluster_texts.mean(axis=0)\n",
    "    top_keywords = [\n",
    "        vectorizer.get_feature_names_out()[j]\n",
    "        for j in mean_tfidf.argsort()[0, -5:].tolist()[0][::-1]\n",
    "    ]\n",
    "    keywords_per_theme.append(\", \".join(top_keywords))\n",
    "\n",
    "# Map cluster to theme name\n",
    "theme_df[\"theme_name\"] = theme_df[\"theme_cluster\"].map({\n",
    "    i: f\"Theme {i+1}: {keywords_per_theme[i]}\" for i in range(NUM_CLUSTERS)\n",
    "})\n",
    "\n",
    "# Merge themes back into original df\n",
    "df_with_theme = df.merge(theme_df, how=\"left\", left_on=\"ai_caption\", right_on=\"caption\")\n",
    "df_with_theme = df_with_theme.drop(columns=[\"caption\"])\n",
    "\n",
    "# Save\n",
    "df_with_theme.to_csv(\"semantic_similarity_with_themes.csv\", index=False)\n",
    "print(\"✅ Saved: semantic_similarity_with_themes.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1493c2-0948-42d8-9e02-ac40b2c091ff",
   "metadata": {},
   "source": [
    "Did not work in a sense that themes aren't relevant "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1bf103-ea02-49f6-8c59-39636cdc9f4d",
   "metadata": {},
   "source": [
    "# zero-shot classification from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c6081f-0d11-441a-b47b-7d79b000f45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbfc88c08c944018d0cf6b4d91b3ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ef4bd4ca534578a36975224b1eae71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c309dacbcb24c19952a76655cb87814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3fbb06bcc64c24bea2871531c56a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38140368c93045e59048d305920dc893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebd43e5a0294edbb1c4bad2af144640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "100%|█████████████████████████████████████| 3392/3392 [27:07<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: semantic_similarity_with_clean_themes.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"semantic_similarity_with_matched_category.csv\")\n",
    "\n",
    "# Combine human + AI captions\n",
    "all_captions = pd.concat([df[\"human_caption\"], df[\"ai_caption\"]]).dropna().unique().tolist()\n",
    "\n",
    "# Define candidate labels (themes)\n",
    "candidate_labels = [\n",
    "    \"religion\", \"politics\", \"family\", \"love\", \"work\", \"money\", \n",
    "    \"animals\", \"technology\", \"psychology\", \"health\", \"death\", \n",
    "    \"relationships\", \"law\", \"parenting\", \"marriage\", \"social media\", \"sports\"\n",
    "]\n",
    "\n",
    "# Load zero-shot classifier\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Classify each caption\n",
    "theme_results = []\n",
    "for caption in tqdm(all_captions):\n",
    "    output = classifier(caption, candidate_labels)\n",
    "    top_theme = output[\"labels\"][0]\n",
    "    theme_results.append({\"caption\": caption, \"theme\": top_theme})\n",
    "\n",
    "# Merge themes back to original dataframe\n",
    "theme_df = pd.DataFrame(theme_results)\n",
    "df_with_themes = df.merge(theme_df, how=\"left\", left_on=\"ai_caption\", right_on=\"caption\")\n",
    "df_with_themes = df_with_themes.drop(columns=[\"caption\"])  # drop temp join column\n",
    "\n",
    "# Save to CSV\n",
    "df_with_themes.to_csv(\"semantic_similarity_with_clean_themes.csv\", index=False)\n",
    "print(\"✅ Saved: semantic_similarity_with_clean_themes.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1e3ec-8cf3-416f-8f08-589b689378d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
