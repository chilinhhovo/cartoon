{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504637ac-6cac-4124-9079-c504079502be",
   "metadata": {},
   "source": [
    "## Add name of dashboard to each csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a859ff68-db8b-4208-a4bc-84c1fcceb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder with all your updated CSV files\n",
    "folder_path = \"dashboards/updated_data\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\") and filename.startswith(\"updated_\"):\n",
    "        # Extract dashboard ID (e.g. \"510\" from \"updated_510 Dashboard.csv\")\n",
    "        dashboard_id = filename.replace(\"updated_\", \"\").split()[0]\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Add or update the 'contest' column\n",
    "        df[\"contest\"] = dashboard_id  # overwrite if it already exists\n",
    "\n",
    "        # Move 'contest' to the first column\n",
    "        cols = df.columns.tolist()\n",
    "        if cols[0] != \"contest\":\n",
    "            cols = [\"contest\"] + [col for col in cols if col != \"contest\"]\n",
    "            df = df[cols]\n",
    "\n",
    "        # Save back\n",
    "        df.to_csv(file_path, index=False)\n",
    "        #print(f\"✅ Processed: {filename} (contest = {dashboard_id})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e1954-1ce2-4e43-87ee-58d572ff16c7",
   "metadata": {},
   "source": [
    "## Combine all of dashboards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "773ce7a6-3b0d-4a41-bb70-d72183254529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined CSV saved as 'all_updated_captions.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder with all your CSVs\n",
    "folder_path = \"dashboards/updated_data\"\n",
    "\n",
    "# List to hold each DataFrame\n",
    "all_dfs = []\n",
    "\n",
    "# Loop through files and append to list\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\") and filename.startswith(\"updated_\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_dfs.append(df)\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Save to a new file\n",
    "combined_df.to_csv(\"all_updated_captions.csv\", index=False)\n",
    "print(\"✅ Combined CSV saved as 'all_updated_captions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5093c0c8-520e-4bbb-8970-8f5fcea0436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s2/1r_lbpts7pvcd7p7h17w62tm0000gn/T/ipykernel_58455/3870191993.py:1: DtypeWarning: Columns (9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv('all_updated_captions.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contest</th>\n",
       "      <th>rank</th>\n",
       "      <th>caption</th>\n",
       "      <th>mean</th>\n",
       "      <th>precision</th>\n",
       "      <th>votes</th>\n",
       "      <th>not_funny</th>\n",
       "      <th>somewhat_funny</th>\n",
       "      <th>funny</th>\n",
       "      <th>has_question_mark</th>\n",
       "      <th>ends_with_punctuation</th>\n",
       "      <th>caption_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I can hear the wife now: \"You caught it, you c...</td>\n",
       "      <td>1.932124</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>5569</td>\n",
       "      <td>2014</td>\n",
       "      <td>1919</td>\n",
       "      <td>1636</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>615</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Must be another Trump hotel that's underwater</td>\n",
       "      <td>1.885976</td>\n",
       "      <td>0.020245</td>\n",
       "      <td>1640</td>\n",
       "      <td>655</td>\n",
       "      <td>517</td>\n",
       "      <td>468</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>615</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I've finally caught something my wife will let...</td>\n",
       "      <td>1.885218</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>5358</td>\n",
       "      <td>1993</td>\n",
       "      <td>1987</td>\n",
       "      <td>1378</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>3.0</td>\n",
       "      <td>\"I hate fishing in the Hamptons\".</td>\n",
       "      <td>1.880827</td>\n",
       "      <td>0.010770</td>\n",
       "      <td>5513</td>\n",
       "      <td>2130</td>\n",
       "      <td>1910</td>\n",
       "      <td>1473</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>615</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Too bad it's catch and release. My wife would ...</td>\n",
       "      <td>1.859643</td>\n",
       "      <td>0.010407</td>\n",
       "      <td>5600</td>\n",
       "      <td>2146</td>\n",
       "      <td>2094</td>\n",
       "      <td>1360</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>615</td>\n",
       "      <td>5.0</td>\n",
       "      <td>We must be at Pier 1.</td>\n",
       "      <td>1.855731</td>\n",
       "      <td>0.010731</td>\n",
       "      <td>5566</td>\n",
       "      <td>2243</td>\n",
       "      <td>1883</td>\n",
       "      <td>1440</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>615</td>\n",
       "      <td>6.0</td>\n",
       "      <td>You're gonna need a nicer boat.</td>\n",
       "      <td>1.843970</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>4544</td>\n",
       "      <td>1866</td>\n",
       "      <td>1521</td>\n",
       "      <td>1157</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>615</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Finally, something my wife will let me mount i...</td>\n",
       "      <td>1.837434</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>5487</td>\n",
       "      <td>2171</td>\n",
       "      <td>2037</td>\n",
       "      <td>1279</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>615</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Those fish are having a ball down there.</td>\n",
       "      <td>1.837133</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>3334</td>\n",
       "      <td>1345</td>\n",
       "      <td>1187</td>\n",
       "      <td>802</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>615</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Now I wish I hadn't thrown back that Chippenda...</td>\n",
       "      <td>1.823428</td>\n",
       "      <td>0.018685</td>\n",
       "      <td>1733</td>\n",
       "      <td>704</td>\n",
       "      <td>631</td>\n",
       "      <td>398</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contest  rank                                            caption      mean  \\\n",
       "0      615   0.0  I can hear the wife now: \"You caught it, you c...  1.932124   \n",
       "1      615   1.0      Must be another Trump hotel that's underwater  1.885976   \n",
       "2      615   2.0  I've finally caught something my wife will let...  1.885218   \n",
       "3      615   3.0                  \"I hate fishing in the Hamptons\".  1.880827   \n",
       "4      615   4.0  Too bad it's catch and release. My wife would ...  1.859643   \n",
       "5      615   5.0                              We must be at Pier 1.  1.855731   \n",
       "6      615   6.0                    You're gonna need a nicer boat.  1.843970   \n",
       "7      615   7.0  Finally, something my wife will let me mount i...  1.837434   \n",
       "8      615   8.0           Those fish are having a ball down there.  1.837133   \n",
       "9      615   9.0  Now I wish I hadn't thrown back that Chippenda...  1.823428   \n",
       "\n",
       "   precision  votes  not_funny  somewhat_funny  funny has_question_mark  \\\n",
       "0   0.010811   5569       2014            1919   1636             False   \n",
       "1   0.020245   1640        655             517    468             False   \n",
       "2   0.010723   5358       1993            1987   1378             False   \n",
       "3   0.010770   5513       2130            1910   1473             False   \n",
       "4   0.010407   5600       2146            2094   1360             False   \n",
       "5   0.010731   5566       2243            1883   1440             False   \n",
       "6   0.011878   4544       1866            1521   1157             False   \n",
       "7   0.010478   5487       2171            2037   1279             False   \n",
       "8   0.013611   3334       1345            1187    802             False   \n",
       "9   0.018685   1733        704             631    398             False   \n",
       "\n",
       "  ends_with_punctuation  caption_length  \n",
       "0                 False            55.0  \n",
       "1                 False            45.0  \n",
       "2                  True            74.0  \n",
       "3                  True            33.0  \n",
       "4                  True            56.0  \n",
       "5                  True            21.0  \n",
       "6                  True            31.0  \n",
       "7                  True            64.0  \n",
       "8                  True            40.0  \n",
       "9                  True            55.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('all_updated_captions.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a1d9a92-a4a9-4e78-8f7d-86fc833d75f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s2/1r_lbpts7pvcd7p7h17w62tm0000gn/T/ipykernel_58455/1153709775.py:1: DtypeWarning: Columns (9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"all_updated_captions.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Added 'model' column with value 'Human'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"all_updated_captions.csv\")\n",
    "\n",
    "df[\"model\"] = \"Human\"\n",
    "df.to_csv(\"all_updated_captions.csv\", index=False)\n",
    "print(\" Added 'model' column with value 'Human'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e24ea9-d076-4b99-9334-7ef89ae244d9",
   "metadata": {},
   "source": [
    "## Combine only top 5 and bottom 5s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a22fb246-a66d-465f-9103-c5deec5fca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"dashboards/top_captions\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # ✅ Clean 'contest' column: remove ' Dashboard'\n",
    "        df[\"contest\"] = df[\"contest\"].astype(str).str.replace(\" Dashboard\", \"\", regex=False)\n",
    "\n",
    "        # ✅ Add NLP feature columns\n",
    "        df[\"has_question_mark\"] = df[\"caption\"].astype(str).str.contains(r\"\\?\")\n",
    "        df[\"ends_with_punctuation\"] = df[\"caption\"].astype(str).str.extract(r\"([\\.\\!\\?])$\").notnull()\n",
    "        df[\"caption_length\"] = df[\"caption\"].astype(str).str.strip().str.len()\n",
    "\n",
    "        # ✅ Add 'model' column with label 'Human'\n",
    "        df[\"model\"] = \"Human\"\n",
    "\n",
    "        # ✅ Save back\n",
    "        df.to_csv(file_path, index=False)\n",
    "        #print(f\"✅ Processed: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad947efa-b01f-4316-a86c-bb1c45b29f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"dashboards/top_captions\"\n",
    "combined = []\n",
    "\n",
    "# Read and collect all CSVs\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined.append(df)\n",
    "\n",
    "# Combine them into one DataFrame\n",
    "merged_df = pd.concat(combined, ignore_index=True)\n",
    "\n",
    "# 🔢 Convert contest to numeric (just in case it's a string)\n",
    "merged_df[\"contest\"] = pd.to_numeric(merged_df[\"contest\"], errors=\"coerce\")\n",
    "\n",
    "# 📈 Sort by contest number\n",
    "merged_df = merged_df.sort_values(by=\"contest\").reset_index(drop=True)\n",
    "\n",
    "# Save sorted version\n",
    "merged_df.to_csv(\"all_top_captions_sorted.csv\", index=False)\n",
    "print(\"✅ Saved sorted file as 'all_top_captions_sorted.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82f7c2-c428-489a-bd64-d51d4e57d588",
   "metadata": {},
   "source": [
    "## AI models and top captions wide format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286075ea-b3a7-40a3-bca5-c9435b6b5d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Long-format CSV saved as 'combined_captions_long_format.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load files\n",
    "human_df = pd.read_csv(\"all_top_captions_sorted.csv\")\n",
    "chatgpt_df = pd.read_csv(\"chatgpt_captions.csv\")\n",
    "claude_df = pd.read_csv(\"claude_captions.csv\")\n",
    "\n",
    "# ========== Ensure expected columns ==========\n",
    "\n",
    "# If missing, compute caption length for ChatGPT and Claude\n",
    "if \"caption_length\" not in chatgpt_df.columns:\n",
    "    chatgpt_df[\"caption_length\"] = chatgpt_df[\"caption\"].str.len()\n",
    "\n",
    "if \"caption_length\" not in claude_df.columns:\n",
    "    claude_df[\"caption_length\"] = claude_df[\"caption\"].str.len()\n",
    "\n",
    "# If punctuation or question mark flags are missing, fill with None\n",
    "for df in [chatgpt_df, claude_df]:\n",
    "    for col in [\"has_question_mark\", \"ends_with_punctuation\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "# ========== Format each source ==========\n",
    "\n",
    "# Human\n",
    "human_df = human_df[[\n",
    "    \"contest\", \"caption\", \"model\", \"has_question_mark\", \"ends_with_punctuation\", \"caption_length\",\n",
    "    \"funny_votes\", \"unfunny_votes\", \"votes\", \"category\"\n",
    "]].copy()\n",
    "human_df[\"source\"] = \"human\"\n",
    "\n",
    "# ChatGPT\n",
    "chatgpt_df = chatgpt_df[[\n",
    "    \"contest\", \"caption\", \"model\", \"has_question_mark\", \"ends_with_punctuation\", \"caption_length\"\n",
    "]].copy()\n",
    "chatgpt_df[\"source\"] = \"chatgpt\"\n",
    "chatgpt_df[\"funny_votes\"] = None\n",
    "chatgpt_df[\"unfunny_votes\"] = None\n",
    "chatgpt_df[\"votes\"] = None\n",
    "chatgpt_df[\"category\"] = None\n",
    "\n",
    "# Claude\n",
    "claude_df = claude_df[[\n",
    "    \"contest\", \"caption\", \"model\", \"has_question_mark\", \"ends_with_punctuation\", \"caption_length\"\n",
    "]].copy()\n",
    "claude_df[\"source\"] = \"claude\"\n",
    "claude_df[\"funny_votes\"] = None\n",
    "claude_df[\"unfunny_votes\"] = None\n",
    "claude_df[\"votes\"] = None\n",
    "claude_df[\"category\"] = None\n",
    "\n",
    "# ========== Combine into long format ==========\n",
    "\n",
    "long_df = pd.concat([human_df, chatgpt_df, claude_df], ignore_index=True)\n",
    "\n",
    "# Save to file\n",
    "long_df.to_csv(\"combined_captions_long_format.csv\", index=False)\n",
    "print(\"✅ Long-format CSV saved as 'combined_captions_long_format.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19d5868-2bec-4551-9252-92ce1b28b4f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'combined_captions_comparison.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcombined_captions_comparison.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'combined_captions_comparison.csv'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('combined_captions_comparison.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d40c3bc-1fb9-4efa-ba75-d752369af899",
   "metadata": {},
   "source": [
    "## AI models and top captions long format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf3f08f4-258f-488c-8f8d-0573d204d892",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['caption_human', 'model_human', 'has_question_mark_human', 'ends_with_punctuation_human', 'caption_length_human'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_captions_long_format.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Build separate DataFrames for each model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m human \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcaption_human\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_human\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_question_mark_human\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mends_with_punctuation_human\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcaption_length_human\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunny_votes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munfunny_votes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvotes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_human\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_human\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_question_mark_human\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_question_mark\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mends_with_punctuation_human\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mends_with_punctuation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_length_human\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_length\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m })\n\u001b[1;32m     19\u001b[0m chatgpt \u001b[38;5;241m=\u001b[39m df[[\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_chatgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_chatgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_question_mark_chatgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mends_with_punctuation_chatgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_length_chatgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_length_chatgpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_length\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m })\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Fill missing human-only columns with NaN\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['caption_human', 'model_human', 'has_question_mark_human', 'ends_with_punctuation_human', 'caption_length_human'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your combined wide-format CSV\n",
    "df = pd.read_csv(\"combined_captions_long_format.csv\")\n",
    "\n",
    "# Build separate DataFrames for each model\n",
    "human = df[[\n",
    "    \"contest\", \"caption_human\", \"model_human\",\n",
    "    \"has_question_mark_human\", \"ends_with_punctuation_human\", \"caption_length_human\",\n",
    "    \"funny_votes\", \"unfunny_votes\", \"votes\", \"category\"\n",
    "]].rename(columns={\n",
    "    \"caption_human\": \"caption\",\n",
    "    \"model_human\": \"model\",\n",
    "    \"has_question_mark_human\": \"has_question_mark\",\n",
    "    \"ends_with_punctuation_human\": \"ends_with_punctuation\",\n",
    "    \"caption_length_human\": \"caption_length\"\n",
    "})\n",
    "\n",
    "chatgpt = df[[\n",
    "    \"contest\", \"caption_chatgpt\", \"model_chatgpt\",\n",
    "    \"has_question_mark_chatgpt\", \"ends_with_punctuation_chatgpt\", \"caption_length_chatgpt\"\n",
    "]].rename(columns={\n",
    "    \"caption_chatgpt\": \"caption\",\n",
    "    \"model_chatgpt\": \"model\",\n",
    "    \"has_question_mark_chatgpt\": \"has_question_mark\",\n",
    "    \"ends_with_punctuation_chatgpt\": \"ends_with_punctuation\",\n",
    "    \"caption_length_chatgpt\": \"caption_length\"\n",
    "})\n",
    "# Fill missing human-only columns with NaN\n",
    "chatgpt[\"funny_votes\"] = None\n",
    "chatgpt[\"unfunny_votes\"] = None\n",
    "chatgpt[\"votes\"] = None\n",
    "chatgpt[\"category\"] = None\n",
    "\n",
    "claude = df[[\n",
    "    \"contest\", \"caption_claude\", \"model_claude\",\n",
    "    \"has_question_mark_claude\", \"ends_with_punctuation_claude\", \"caption_length_claude\"\n",
    "]].rename(columns={\n",
    "    \"caption_claude\": \"caption\",\n",
    "    \"model_claude\": \"model\",\n",
    "    \"has_question_mark_claude\": \"has_question_mark\",\n",
    "    \"ends_with_punctuation_claude\": \"ends_with_punctuation\",\n",
    "    \"caption_length_claude\": \"caption_length\"\n",
    "})\n",
    "claude[\"funny_votes\"] = None\n",
    "claude[\"unfunny_votes\"] = None\n",
    "claude[\"votes\"] = None\n",
    "claude[\"category\"] = None\n",
    "\n",
    "# Combine into long format\n",
    "long_df = pd.concat([human, chatgpt, claude], ignore_index=True)\n",
    "\n",
    "# Save it\n",
    "long_df.to_csv(\"captions_long_format.csv\", index=False)\n",
    "print(\"✅ Saved long-format file as 'captions_long_format.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d945c7d-fed1-4dd4-bf25-2b4027c92041",
   "metadata": {},
   "source": [
    "# combine with semantic analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24983f80-51a9-43eb-b672-cbb0aa173e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV updated with SVG paths and saved to:\n",
      "semantic_similarity_with_svg_path.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_dir = \"new_yorker_contest\"\n",
    "csv_path = os.path.join(\"semantic_similarity_with_matched_category.csv\")\n",
    "svg_folder = os.path.join(\"similarity_values\")\n",
    "output_path = os.path.join(\"semantic_similarity_with_svg_path.csv\")\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Get all SVG filenames and map them to contest numbers\n",
    "svg_map = {\n",
    "    int(os.path.splitext(f)[0]): os.path.join(svg_folder, f)\n",
    "    for f in os.listdir(svg_folder)\n",
    "    if f.endswith(\".svg\") and os.path.splitext(f)[0].isdigit()\n",
    "}\n",
    "\n",
    "# Add SVG path to DataFrame based on contest number\n",
    "df[\"svg_path\"] = df[\"contest\"].map(svg_map)\n",
    "\n",
    "# Save the updated CSV\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"✅ CSV updated with SVG paths and saved to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a707ab88-c894-4236-a570-10d53df0b34b",
   "metadata": {},
   "source": [
    "## combine svg, theme + remove Claude's extra words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e86d7b6-c72c-4b39-9738-6ee752a93d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined file saved at:\n",
      "semantic_similarity_combined_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths (edit if yours differ)\n",
    "themes_path = \"semantic_similarity_with_clean_themes.csv\"\n",
    "svg_path = \"semantic_similarity_with_svg_path.csv\"\n",
    "output_path = \"semantic_similarity_combined_final.csv\"\n",
    "\n",
    "# Load both CSVs\n",
    "df_themes = pd.read_csv(themes_path)\n",
    "df_svg = pd.read_csv(svg_path)\n",
    "\n",
    "# Merge on 'contest' and 'ai_caption'\n",
    "df_merged = pd.merge(\n",
    "    df_themes,\n",
    "    df_svg[[\"contest\", \"ai_caption\", \"svg_path\"]],\n",
    "    on=[\"contest\", \"ai_caption\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Clean Claude captions: strip everything after \"The image\"\n",
    "def clean_claude_caption(row):\n",
    "    if row[\"ai_model\"] == \"Claude\":\n",
    "        return re.split(r\"\\bThe image\\b\", str(row[\"ai_caption\"]))[0].strip()\n",
    "    return row[\"ai_caption\"]\n",
    "\n",
    "df_merged[\"ai_caption\"] = df_merged.apply(clean_claude_caption, axis=1)\n",
    "\n",
    "# Save result\n",
    "df_merged.to_csv(output_path, index=False)\n",
    "print(f\"✅ Combined file saved at:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ca2fdb3-0bf2-452f-a8cf-f52f7ef9e070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned file saved to: semantic_similarity_with_svg_path.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV\n",
    "file_path = \"semantic_similarity_with_svg_path.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define a function to clean Claude captions\n",
    "def clean_claude_caption(caption):\n",
    "    if pd.isna(caption):\n",
    "        return caption\n",
    "    return caption.split(\" The\")[0].strip()  # Remove everything from \" The\"\n",
    "\n",
    "# Apply only to Claude rows\n",
    "df.loc[df[\"ai_model\"] == \"Claude\", \"ai_caption\"] = df.loc[df[\"ai_model\"] == \"Claude\", \"ai_caption\"].apply(clean_claude_caption)\n",
    "\n",
    "# Save the cleaned file\n",
    "output_path = \"semantic_similarity_with_svg_path.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc4c0b8-5864-4ddd-8eb6-685440d5814a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
